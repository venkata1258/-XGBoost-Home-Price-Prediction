# -*- coding: utf-8 -*-
"""XGBoost Home Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t7zUsSnJ79CA7NXnDnDk_JQMH7ijw12L
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import matplotlib.pyplot as plt

# Load your dataset
data = pd.read_csv('/content/kc_house_data.csv')

# Handle missing values for numeric features only
numeric_features = data.select_dtypes(include=np.number).columns
data[numeric_features] = data[numeric_features].fillna(data[numeric_features].mean())

# Select features and target variable
# Replaced placeholders with likely column names based on kc_house_data
X = data[['sqft_living', 'bedrooms', 'bathrooms', 'grade']]
y = data['price']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create DMatrix objects
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set XGBoost parameters
params = {
    'objective': 'reg:squarederror',
    'max_depth': 3,
    'eta': 0.1,
    'eval_metric': 'rmse'
}

# Train the model
num_rounds = 100
model = xgb.train(params, dtrain, num_rounds)

# Make predictions
y_pred = model.predict(dtest)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error: {rmse}")

# Get feature importance
importance = model.get_score(importance_type='gain')

# Visualize feature importance
plt.bar(range(len(importance)), list(importance.values()), align='center')
plt.xticks(range(len(importance)), list(importance.keys()))
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('XGBoost Feature Importance')
plt.show()